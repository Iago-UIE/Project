{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67f0c0dc",
   "metadata": {},
   "source": [
    "Parte 1\n",
    "\n",
    "Preprocesamientos no otorgados por keras\n",
    "\n",
    "\n",
    "transfromaciÃ³n a tensores, entendibles por torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "724e930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import csv\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "198a2340",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Transformaciones\n",
    "transform_im = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normaliza a [-1, 1]\n",
    "])\n",
    "\n",
    "\n",
    "#Preprocesamiento para texto\n",
    "# Diccionario de mapeo letra -> Ã­ndice (0 a 51)\n",
    "all_letters = list(string.ascii_uppercase + string.ascii_lowercase)\n",
    "letter_to_idx = {char: idx for idx, char in enumerate(all_letters)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c130689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listas para acumular los datos\n",
    "img_list = []\n",
    "label_values = []  # Cambiado el nombre para mayor claridad\n",
    "\n",
    "CSV_PATH = \"arial_dataset/arial_letters.csv\"\n",
    "IMG_DIR = \"arial_dataset\"\n",
    "\n",
    "with open(CSV_PATH, \"r\", encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Saltar encabezado\n",
    "\n",
    "    for row in reader:\n",
    "        img_name, label = row\n",
    "        img_path = os.path.join(IMG_DIR, img_name)\n",
    "\n",
    "        # Procesar imagen\n",
    "        img = Image.open(img_path)\n",
    "        img_list.append(transform_im(img))\n",
    "\n",
    "        # Procesar etiqueta\n",
    "        label_idx = letter_to_idx[label]\n",
    "        label_values.append(label_idx)\n",
    "\n",
    "# Convertir a tensores al final\n",
    "img_tensor = torch.stack(img_list)  # Tensor de imÃ¡genes\n",
    "text_tensor = torch.tensor(label_values, dtype=torch.long)  # Tensor de etiquetas numÃ©ricas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75333393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgtensor: \n",
      "\n",
      " tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]]]]) \n",
      "\n",
      "\n",
      "size im: n\n",
      " torch.Size([5200, 1, 128, 128]) \n",
      "\n",
      "\n",
      "Texttensor: \n",
      "\n",
      " tensor([ 0,  0,  0,  ..., 25, 25, 25])\n"
     ]
    }
   ],
   "source": [
    "print(\"imgtensor: \\n\\n\", img_tensor, \"\\n\\n\")\n",
    "print(\"size im: n\\n\", img_tensor.shape, \"\\n\\n\")\n",
    "print(\"Texttensor: \\n\\n\",text_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9035c4b",
   "metadata": {},
   "source": [
    "Dividiendo los datos de entrenamiento y guardÃ¡ndolos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52a662eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b446848",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(img_tensor, text_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# Guardar los tensores\n",
    "torch.save({\n",
    "    'X_train': X_train,\n",
    "    'X_test': X_test,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test\n",
    "}, 'train_test_split.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ccc4108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iagoc\\AppData\\Local\\Temp\\ipykernel_5392\\737453814.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(\"train_test_split.pt\")\n"
     ]
    }
   ],
   "source": [
    "# Carga el contenido del archivo\n",
    "data = torch.load(\"train_test_split.pt\")\n",
    "\n",
    "# Accede a los datos si estÃ¡n organizados en un diccionario\n",
    "X_train = data[\"X_train\"]\n",
    "X_test = data[\"X_test\"]\n",
    "y_train = data[\"y_train\"]\n",
    "y_test = data[\"y_test\"]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2305bb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7ccf9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LetterClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=26):\n",
    "        super(LetterClassifier, self).__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),  # 1x128x128 â†’ 32x128x128\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                             # 32x64x64\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), # 64x64x64\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                             # 64x32x32\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),# 128x32x32\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                             # 128x16x16\n",
    "        )\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(128 * 16 * 16, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc_layers(x)\n",
    "        return x  # logits (sin softmax, para usar con CrossEntropyLoss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d8e2df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# DataLoader procesamiento por batches, mezcla aleatoriamente los datos y carga los datos en paralelo\n",
    "from torch.utils.data import TensorDataset \n",
    "# TensorDataset crea un dataset a partir de varios tensores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71d3b286",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376faa45",
   "metadata": {},
   "source": [
    "Definimos la funciÃ³n de pÃ©rdida y el optimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f06c3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71bc292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LetterClassifier(num_classes=52)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c2ef46",
   "metadata": {},
   "source": [
    "Iniciamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09d3e00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/15 | LR=0.000500 | Loss=4.9515 | MejGlobal=4.9515\n",
      "Iter 2/15 | LR=0.000400 | Loss=3.5501 | MejGlobal=3.5501\n",
      "Iter 3/15 | LR=0.000500 | Loss=2.7955 | MejGlobal=2.7955\n",
      "Iter 4/15 | LR=0.000400 | Loss=1.9177 | MejGlobal=1.9177\n",
      "Iter 5/15 | LR=0.000300 | Loss=1.9122 | MejGlobal=1.9122\n",
      "Iter 6/15 | LR=0.000200 | Loss=1.6822 | MejGlobal=1.6822\n",
      "Iter 7/15 | LR=0.000100 | Loss=1.7181 | MejGlobal=1.6822\n",
      "Iter 8/15 | LR=0.000600 | Loss=4.4605 | MejGlobal=1.6822\n",
      "Iter 9/15 | LR=0.000100 | Loss=1.5817 | MejGlobal=1.5817\n",
      "Iter 10/15 | LR=0.000200 | Loss=1.5691 | MejGlobal=1.5691\n",
      "Iter 11/15 | LR=0.000300 | Loss=1.2457 | MejGlobal=1.2457\n",
      "Iter 12/15 | LR=0.000200 | Loss=1.5643 | MejGlobal=1.2457\n",
      "Iter 13/15 | LR=0.000100 | Loss=2.0158 | MejGlobal=1.2457\n",
      "Iter 14/15 | LR=0.000600 | Loss=7.2393 | MejGlobal=1.2457\n",
      "Iter 15/15 | LR=0.000100 | Loss=2.0696 | MejGlobal=1.2457\n",
      "\n",
      "=== Mejor configuraciÃ³n encontrada ===\n",
      "{'lr': 0.00030000000000000003, 'num_epochs': 5}\n",
      "Loss acumulado = 1.2457\n",
      "\n",
      "=== Resumen de LONG-TERM MEMORY ===\n",
      "{'visits': {0.001: 3, 0.0005: 5, 0.0009: 3, 0.0011: 3, 0.0015: 1, 1e-06: 15, 0.0004: 4, 0.0006000000000000001: 5, 0.00030000000000000003: 5, 0.00020000000000000004: 5, 0.0008: 2, 0.00010000000000000003: 3, 0.0007000000000000001: 5, 0.00010000000000000005: 2}, 'best_scores': {0.001: 11.769389686512568, 0.0005: 2.795458361373676, 0.0009: 16.448553548238124, 0.0011: 17.71386744872143, 0.0015: 27.578365669436607, 1e-06: 20.84701324647176, 0.0004: 1.8314993314013464, 0.0006000000000000001: 4.460475637436957, 0.00030000000000000003: 1.2456788033059638, 0.00020000000000000004: 1.564254334411089, 0.0008: 12.232156936364845, 0.00010000000000000003: 1.7181020756961516, 0.0007000000000000001: 5.994350398537563, 0.00010000000000000005: 1.5817088985659211}}\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import random\n",
    "import string\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# â€”â€”â€”â€”â€”â€” ConfiguraciÃ³n inicial de tu red y datos â€”â€”â€”â€”â€”â€”\n",
    "# Asume que `model`, `criterion`, `train_loader` estÃ¡n definidos en tu entorno.\n",
    "# Por ejemplo:\n",
    "# model = MyNeuralNet()\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# train_loader = DataLoader(mi_dataset, batch_size=32)\n",
    "\n",
    "# Diccionario para imprimir Ã­ndices como letras\n",
    "idx_to_letter = list(string.ascii_uppercase + string.ascii_lowercase)\n",
    "def tensor_a_letras(tensor):\n",
    "    return [idx_to_letter[idx.item()] for idx in tensor]\n",
    "\n",
    "# â€”â€”â€”â€”â€”â€” FunciÃ³n de evaluaciÃ³n â€”â€”â€”â€”â€”â€”\n",
    "# Recibe una configuraciÃ³n de hiperparÃ¡metros y devuelve el loss total entrenando el modelo clonado.\n",
    "def evaluar_loss(config):\n",
    "    # CLONAMOS modelo original para no modificarlo\n",
    "    modelo_copia = copy.deepcopy(model)\n",
    "    # DEFINIMOS optimizador Adam con los parÃ¡metros de 'config'\n",
    "    optimizer = optim.Adam(\n",
    "        modelo_copia.parameters(),\n",
    "        lr=config['lr'],\n",
    "        betas=config.get('betas', (0.9, 0.999)),\n",
    "        eps=config.get('eps', 1e-8),\n",
    "        weight_decay=config.get('weight_decay', 0.0),\n",
    "        amsgrad=config.get('amsgrad', False)\n",
    "    )\n",
    "    num_epochs = config.get('num_epochs', 3)\n",
    "    total_loss = 0.0\n",
    "\n",
    "    modelo_copia.train()\n",
    "    for _ in range(num_epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = modelo_copia(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "    return total_loss\n",
    "\n",
    "# â€”â€”â€”â€”â€”â€” Generador de vecindario â€”â€”â€”â€”â€”â€”\n",
    "def generar_vecindario(sol_actual):\n",
    "    \"\"\"\n",
    "    Crea vecinos variando ligeramente el lr.\n",
    "    AquÃ­ solo variamos LR, pero podrÃ­as incluir betas, weight_decay, etc.\n",
    "    \"\"\"\n",
    "    deltas = [-0.0005, -0.0001, 0.0001, 0.0005]\n",
    "    vecinos = []\n",
    "    for d in deltas:\n",
    "        lr_nuevo = max(1e-6, sol_actual['lr'] + d)\n",
    "        vecino = sol_actual.copy()\n",
    "        vecino['lr'] = lr_nuevo\n",
    "        vecinos.append(vecino)\n",
    "    return vecinos\n",
    "\n",
    "# â€”â€”â€”â€”â€”â€” Tabu Search â€”â€”â€”â€”â€”â€”\n",
    "# AquÃ­ implementamos:\n",
    "#  â€¢ SHORT-TERM MEMORY: tabu_list evita revisitar movimientos prohibidos.\n",
    "#  â€¢ LONG-TERM MEMORY: long_term registra estadÃ­sticas de exploraciÃ³n.\n",
    "#  â€¢ ASPIRATION CRITERION: permite aceptar un movimiento tabu si mejora el mejor global.\n",
    "\n",
    "def tabu_search(iters=20, tabu_size=5, initial_config=None):\n",
    "    # 1) ConfiguraciÃ³n inicial\n",
    "    if initial_config is None:\n",
    "        current = {'lr': random.uniform(1e-4, 1e-2), 'num_epochs': 3}\n",
    "    else:\n",
    "        current = initial_config.copy()\n",
    "\n",
    "    # Evaluar estado inicial\n",
    "    best = current.copy()\n",
    "    best_score = evaluar_loss(best)\n",
    "\n",
    "    # SHORT-TERM MEMORY\n",
    "    tabu_list = []  # movimientos prohibidos temporalmente\n",
    "\n",
    "    # LONG-TERM MEMORY\n",
    "    long_term = {\n",
    "        'visits': {},      # cuÃ¡ntas veces se probÃ³ cada lr\n",
    "        'best_scores': {}  # mejor loss por lr\n",
    "    }\n",
    "    lr0 = current['lr']\n",
    "    long_term['visits'][lr0] = 1\n",
    "    long_term['best_scores'][lr0] = best_score\n",
    "\n",
    "    for i in range(iters):\n",
    "        # 2) Generar vecindario reducido (Candidate List Strategies)\n",
    "        #    Para reducir costo, sÃ³lo consideramos un subconjunto de vecinos.\n",
    "        vecinos = generar_vecindario(current)[:4]  # ejemplo: sÃ³lo primeros 4\n",
    "\n",
    "        # 3) Filtrado con Tabu y Criterio de AspiraciÃ³n\n",
    "        candidatos = []\n",
    "        for v in vecinos:\n",
    "            is_tabu = v in tabu_list\n",
    "            score_v = evaluar_loss(v)  # pre-evaluamos breve para aspiraciÃ³n\n",
    "            # Aspiration: si un movimiento tabu mejora el best_score global, aceptamos\n",
    "            if not is_tabu or score_v < best_score:\n",
    "                candidatos.append((score_v, v, is_tabu))\n",
    "            # Actualizar LTM independientemente\n",
    "            lr = v['lr']\n",
    "            long_term['visits'][lr] = long_term['visits'].get(lr, 0) + 1\n",
    "            prev_best = long_term['best_scores'].get(lr, float('inf'))\n",
    "            long_term['best_scores'][lr] = min(prev_best, score_v)\n",
    "\n",
    "        # 4) Seleccionar mejor candidato\n",
    "        candidatos.sort(key=lambda x: x[0])\n",
    "        best_vec_score, best_vec, was_tabu = candidatos[0]\n",
    "\n",
    "        # 5) Actualizar estado\n",
    "        current = best_vec\n",
    "        # Si no se usÃ³ aspiraciÃ³n para este tabu, aÃ±adimos\n",
    "        if not was_tabu:\n",
    "            tabu_list.append(best_vec)\n",
    "            if len(tabu_list) > tabu_size:\n",
    "                tabu_list.pop(0)\n",
    "\n",
    "        # 6) Actualizar mejor global\n",
    "        if best_vec_score < best_score:\n",
    "            best_score = best_vec_score\n",
    "            best = best_vec.copy()\n",
    "\n",
    "        print(f\"Iter {i+1}/{iters} | LR={current['lr']:.6f} | Loss={best_vec_score:.4f} | MejGlobal={best_score:.4f}\")\n",
    "\n",
    "    # 7) Devolver mejor configuraciÃ³n, su score y memoria a largo plazo\n",
    "    return best, best_score, long_term\n",
    "\n",
    "# â€”â€”â€”â€”â€”â€” EjecuciÃ³n â€”â€”â€”â€”â€”â€”\n",
    "if __name__ == \"__main__\":\n",
    "    mejor_conf, mejor_loss, memoria = tabu_search(\n",
    "        iters=15,\n",
    "        tabu_size=4,\n",
    "        initial_config={'lr': 0.001, 'num_epochs': 5}\n",
    "    )\n",
    "    print(\"\\n=== Mejor configuraciÃ³n encontrada ===\")\n",
    "    print(mejor_conf)\n",
    "    print(f\"Loss acumulado = {mejor_loss:.4f}\")\n",
    "    print(\"\\n=== Resumen de LONG-TERM MEMORY ===\")\n",
    "    print(memoria)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00b70b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Accuracy en test: 97.98%\n",
      "\n",
      "ðŸ“‹ ClasificaciÃ³n por clase:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       1.00      0.98      0.99        50\n",
      "           B       0.97      0.95      0.96        41\n",
      "           C       1.00      0.93      0.97        45\n",
      "           D       1.00      0.98      0.99        45\n",
      "           E       1.00      0.95      0.97        38\n",
      "           F       0.90      1.00      0.95        37\n",
      "           G       1.00      0.87      0.93        38\n",
      "           H       1.00      1.00      1.00        45\n",
      "           I       1.00      1.00      1.00        41\n",
      "           J       0.97      1.00      0.99        37\n",
      "           K       1.00      1.00      1.00        26\n",
      "           L       1.00      1.00      1.00        39\n",
      "           M       0.97      1.00      0.99        37\n",
      "           N       0.98      0.95      0.96        42\n",
      "           O       0.95      0.97      0.96        40\n",
      "           P       1.00      0.97      0.99        37\n",
      "           Q       0.96      1.00      0.98        43\n",
      "           R       0.98      1.00      0.99        40\n",
      "           S       0.79      1.00      0.88        26\n",
      "           T       1.00      1.00      1.00        48\n",
      "           U       1.00      1.00      1.00        34\n",
      "           V       0.98      0.98      0.98        49\n",
      "           W       1.00      1.00      1.00        45\n",
      "           X       1.00      1.00      1.00        36\n",
      "           Y       1.00      0.95      0.98        44\n",
      "           Z       1.00      1.00      1.00        37\n",
      "\n",
      "    accuracy                           0.98      1040\n",
      "   macro avg       0.98      0.98      0.98      1040\n",
      "weighted avg       0.98      0.98      0.98      1040\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "model.eval()  # Muy importante: desactiva dropout/batchnorm si los hubiera\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():  # No se calculan gradientes, mÃ¡s eficiente\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# MÃ©tricas\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "print(f\"\\nâœ… Accuracy en test: {acc * 100:.2f}%\")\n",
    "\n",
    "# Reporte mÃ¡s detallado\n",
    "print(\"\\nðŸ“‹ ClasificaciÃ³n por clase:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=list(string.ascii_uppercase)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af4ccb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"letter_classifier.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1dc1592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "942043ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LetterClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=52):\n",
    "        super(LetterClassifier, self).__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),  # 1x128x128 â†’ 32x128x128\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                             # 32x64x64\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), # 64x64x64\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                             # 64x32x32\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),# 128x32x32\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                             # 128x16x16\n",
    "        )\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(128 * 16 * 16, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc_layers(x)\n",
    "        return x  # logits (sin softmax, para usar con CrossEntropyLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c1b72d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iagoc\\AppData\\Local\\Temp\\ipykernel_5392\\1852476355.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  modelo.load_state_dict(torch.load('letter_classifier.pth', map_location='cpu'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LetterClassifier(\n",
       "  (conv_layers): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc_layers): Sequential(\n",
       "    (0): Linear(in_features=32768, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=52, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo = LetterClassifier()\n",
    "modelo.load_state_dict(torch.load('letter_classifier.pth', map_location='cpu'))\n",
    "modelo.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d670319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Letra predicha: A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iagoc\\AppData\\Local\\Temp\\ipykernel_5392\\2694318761.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  modelo.load_state_dict(torch.load(\"letter_classifier.pth\", map_location='cpu'))\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import string\n",
    "\n",
    "# Paso 1: Diccionario para convertir Ã­ndices a letras\n",
    "idx_to_letter = list(string.ascii_uppercase + string.ascii_lowercase)\n",
    "\n",
    "# Paso 2: Cargar y transformar la imagen\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),                  # Asegura 1 canal\n",
    "    transforms.Resize((128, 128)),             # TamaÃ±o esperado\n",
    "    transforms.ToTensor(),                   # Convertir a tensor [0,1]\n",
    "    transforms.Normalize((0.5,), (0.5,))     # NormalizaciÃ³n opcional\n",
    "])\n",
    "\n",
    "# Paso 3: Cargar imagen\n",
    "img_path = \"arial_dataset/A_0.png\"\n",
    "imagen = Image.open(img_path)\n",
    "imagen = transform(imagen)\n",
    "imagen = imagen.unsqueeze(0)  # AÃ±ade dimensiÃ³n de batch: [1, 1, 28, 28]\n",
    "\n",
    "# Paso 4: Cargar el modelo ya corregido\n",
    "modelo = LetterClassifier()\n",
    "modelo.load_state_dict(torch.load(\"letter_classifier.pth\", map_location='cpu'))\n",
    "modelo.eval()\n",
    "\n",
    "# Paso 5: Realizar predicciÃ³n\n",
    "with torch.no_grad():\n",
    "    salida = modelo(imagen)\n",
    "    prediccion = torch.argmax(salida, dim=1).item()\n",
    "    letra = idx_to_letter[prediccion]\n",
    "\n",
    "print(f\"Letra predicha: {letra}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
