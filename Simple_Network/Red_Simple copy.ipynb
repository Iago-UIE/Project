{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67f0c0dc",
   "metadata": {},
   "source": [
    "Parte 1\n",
    "\n",
    "Preprocesamientos no otorgados por keras\n",
    "\n",
    "\n",
    "transfromación a tensores, entendibles por torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "724e930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import csv\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "198a2340",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Transformaciones\n",
    "transform_im = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normaliza a [-1, 1]\n",
    "])\n",
    "\n",
    "\n",
    "#Preprocesamiento para texto\n",
    "# Diccionario de mapeo letra -> índice (0 a 51)\n",
    "all_letters = list(string.ascii_uppercase + string.ascii_lowercase)\n",
    "letter_to_idx = {char: idx for idx, char in enumerate(all_letters)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c130689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listas para acumular los datos\n",
    "img_list = []\n",
    "label_values = []  # Cambiado el nombre para mayor claridad\n",
    "\n",
    "CSV_PATH = \"arial_dataset/arial_letters.csv\"\n",
    "IMG_DIR = \"arial_dataset\"\n",
    "\n",
    "with open(CSV_PATH, \"r\", encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Saltar encabezado\n",
    "\n",
    "    for row in reader:\n",
    "        img_name, label = row\n",
    "        img_path = os.path.join(IMG_DIR, img_name)\n",
    "\n",
    "        # Procesar imagen\n",
    "        img = Image.open(img_path)\n",
    "        img_list.append(transform_im(img))\n",
    "\n",
    "        # Procesar etiqueta\n",
    "        label_idx = letter_to_idx[label]\n",
    "        label_values.append(label_idx)\n",
    "\n",
    "# Convertir a tensores al final\n",
    "img_tensor = torch.stack(img_list)  # Tensor de imágenes\n",
    "text_tensor = torch.tensor(label_values, dtype=torch.long)  # Tensor de etiquetas numéricas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75333393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgtensor: \n",
      "\n",
      " tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]]]]) \n",
      "\n",
      "\n",
      "size im: n\n",
      " torch.Size([5200, 1, 128, 128]) \n",
      "\n",
      "\n",
      "Texttensor: \n",
      "\n",
      " tensor([ 0,  0,  0,  ..., 25, 25, 25])\n"
     ]
    }
   ],
   "source": [
    "print(\"imgtensor: \\n\\n\", img_tensor, \"\\n\\n\")\n",
    "print(\"size im: n\\n\", img_tensor.shape, \"\\n\\n\")\n",
    "print(\"Texttensor: \\n\\n\",text_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9035c4b",
   "metadata": {},
   "source": [
    "Dividiendo los datos de entrenamiento y guardándolos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52a662eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b446848",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(img_tensor, text_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# Guardar los tensores\n",
    "torch.save({\n",
    "    'X_train': X_train,\n",
    "    'X_test': X_test,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test\n",
    "}, 'train_test_split.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ccc4108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iagoc\\AppData\\Local\\Temp\\ipykernel_5392\\737453814.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(\"train_test_split.pt\")\n"
     ]
    }
   ],
   "source": [
    "# Carga el contenido del archivo\n",
    "data = torch.load(\"train_test_split.pt\")\n",
    "\n",
    "# Accede a los datos si están organizados en un diccionario\n",
    "X_train = data[\"X_train\"]\n",
    "X_test = data[\"X_test\"]\n",
    "y_train = data[\"y_train\"]\n",
    "y_test = data[\"y_test\"]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2305bb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7ccf9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LetterClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=26):\n",
    "        super(LetterClassifier, self).__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),  # 1x128x128 → 32x128x128\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                             # 32x64x64\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), # 64x64x64\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                             # 64x32x32\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),# 128x32x32\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                             # 128x16x16\n",
    "        )\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(128 * 16 * 16, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc_layers(x)\n",
    "        return x  # logits (sin softmax, para usar con CrossEntropyLoss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d8e2df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# DataLoader procesamiento por batches, mezcla aleatoriamente los datos y carga los datos en paralelo\n",
    "from torch.utils.data import TensorDataset \n",
    "# TensorDataset crea un dataset a partir de varios tensores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71d3b286",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376faa45",
   "metadata": {},
   "source": [
    "Definimos la función de pérdida y el optimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f06c3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71bc292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LetterClassifier(num_classes=52)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c2ef46",
   "metadata": {},
   "source": [
    "Iniciamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09d3e00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/15 | LR=0.000500 | Loss=4.9515 | MejGlobal=4.9515\n",
      "Iter 2/15 | LR=0.000400 | Loss=3.5501 | MejGlobal=3.5501\n",
      "Iter 3/15 | LR=0.000500 | Loss=2.7955 | MejGlobal=2.7955\n",
      "Iter 4/15 | LR=0.000400 | Loss=1.9177 | MejGlobal=1.9177\n",
      "Iter 5/15 | LR=0.000300 | Loss=1.9122 | MejGlobal=1.9122\n",
      "Iter 6/15 | LR=0.000200 | Loss=1.6822 | MejGlobal=1.6822\n",
      "Iter 7/15 | LR=0.000100 | Loss=1.7181 | MejGlobal=1.6822\n",
      "Iter 8/15 | LR=0.000600 | Loss=4.4605 | MejGlobal=1.6822\n",
      "Iter 9/15 | LR=0.000100 | Loss=1.5817 | MejGlobal=1.5817\n",
      "Iter 10/15 | LR=0.000200 | Loss=1.5691 | MejGlobal=1.5691\n",
      "Iter 11/15 | LR=0.000300 | Loss=1.2457 | MejGlobal=1.2457\n",
      "Iter 12/15 | LR=0.000200 | Loss=1.5643 | MejGlobal=1.2457\n",
      "Iter 13/15 | LR=0.000100 | Loss=2.0158 | MejGlobal=1.2457\n",
      "Iter 14/15 | LR=0.000600 | Loss=7.2393 | MejGlobal=1.2457\n",
      "Iter 15/15 | LR=0.000100 | Loss=2.0696 | MejGlobal=1.2457\n",
      "\n",
      "=== Mejor configuración encontrada ===\n",
      "{'lr': 0.00030000000000000003, 'num_epochs': 5}\n",
      "Loss acumulado = 1.2457\n",
      "\n",
      "=== Resumen de LONG-TERM MEMORY ===\n",
      "{'visits': {0.001: 3, 0.0005: 5, 0.0009: 3, 0.0011: 3, 0.0015: 1, 1e-06: 15, 0.0004: 4, 0.0006000000000000001: 5, 0.00030000000000000003: 5, 0.00020000000000000004: 5, 0.0008: 2, 0.00010000000000000003: 3, 0.0007000000000000001: 5, 0.00010000000000000005: 2}, 'best_scores': {0.001: 11.769389686512568, 0.0005: 2.795458361373676, 0.0009: 16.448553548238124, 0.0011: 17.71386744872143, 0.0015: 27.578365669436607, 1e-06: 20.84701324647176, 0.0004: 1.8314993314013464, 0.0006000000000000001: 4.460475637436957, 0.00030000000000000003: 1.2456788033059638, 0.00020000000000000004: 1.564254334411089, 0.0008: 12.232156936364845, 0.00010000000000000003: 1.7181020756961516, 0.0007000000000000001: 5.994350398537563, 0.00010000000000000005: 1.5817088985659211}}\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import random\n",
    "import string\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# —————— Configuración inicial de tu red y datos ——————\n",
    "# Asume que `model`, `criterion`, `train_loader` están definidos en tu entorno.\n",
    "# Por ejemplo:\n",
    "# model = MyNeuralNet()\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# train_loader = DataLoader(mi_dataset, batch_size=32)\n",
    "\n",
    "# Diccionario para imprimir índices como letras\n",
    "idx_to_letter = list(string.ascii_uppercase + string.ascii_lowercase)\n",
    "def tensor_a_letras(tensor):\n",
    "    return [idx_to_letter[idx.item()] for idx in tensor]\n",
    "\n",
    "# —————— Función de evaluación ——————\n",
    "# Recibe una configuración de hiperparámetros y devuelve el loss total entrenando el modelo clonado.\n",
    "def evaluar_loss(config):\n",
    "    # CLONAMOS modelo original para no modificarlo\n",
    "    modelo_copia = copy.deepcopy(model)\n",
    "    # DEFINIMOS optimizador Adam con los parámetros de 'config'\n",
    "    optimizer = optim.Adam(\n",
    "        modelo_copia.parameters(),\n",
    "        lr=config['lr'],\n",
    "        betas=config.get('betas', (0.9, 0.999)),\n",
    "        eps=config.get('eps', 1e-8),\n",
    "        weight_decay=config.get('weight_decay', 0.0),\n",
    "        amsgrad=config.get('amsgrad', False)\n",
    "    )\n",
    "    num_epochs = config.get('num_epochs', 3)\n",
    "    total_loss = 0.0\n",
    "\n",
    "    modelo_copia.train()\n",
    "    for _ in range(num_epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = modelo_copia(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "    return total_loss\n",
    "\n",
    "# —————— Generador de vecindario ——————\n",
    "def generar_vecindario(sol_actual):\n",
    "    \"\"\"\n",
    "    Crea vecinos variando ligeramente el lr.\n",
    "    Aquí solo variamos LR, pero podrías incluir betas, weight_decay, etc.\n",
    "    \"\"\"\n",
    "    deltas = [-0.0005, -0.0001, 0.0001, 0.0005]\n",
    "    vecinos = []\n",
    "    for d in deltas:\n",
    "        lr_nuevo = max(1e-6, sol_actual['lr'] + d)\n",
    "        vecino = sol_actual.copy()\n",
    "        vecino['lr'] = lr_nuevo\n",
    "        vecinos.append(vecino)\n",
    "    return vecinos\n",
    "\n",
    "# —————— Tabu Search ——————\n",
    "# Aquí implementamos:\n",
    "#  • SHORT-TERM MEMORY: tabu_list evita revisitar movimientos prohibidos.\n",
    "#  • LONG-TERM MEMORY: long_term registra estadísticas de exploración.\n",
    "#  • ASPIRATION CRITERION: permite aceptar un movimiento tabu si mejora el mejor global.\n",
    "\n",
    "def tabu_search(iters=20, tabu_size=5, initial_config=None):\n",
    "    # 1) Configuración inicial\n",
    "    if initial_config is None:\n",
    "        current = {'lr': random.uniform(1e-4, 1e-2), 'num_epochs': 3}\n",
    "    else:\n",
    "        current = initial_config.copy()\n",
    "\n",
    "    # Evaluar estado inicial\n",
    "    best = current.copy()\n",
    "    best_score = evaluar_loss(best)\n",
    "\n",
    "    # SHORT-TERM MEMORY\n",
    "    tabu_list = []  # movimientos prohibidos temporalmente\n",
    "\n",
    "    # LONG-TERM MEMORY\n",
    "    long_term = {\n",
    "        'visits': {},      # cuántas veces se probó cada lr\n",
    "        'best_scores': {}  # mejor loss por lr\n",
    "    }\n",
    "    lr0 = current['lr']\n",
    "    long_term['visits'][lr0] = 1\n",
    "    long_term['best_scores'][lr0] = best_score\n",
    "\n",
    "    for i in range(iters):\n",
    "        # 2) Generar vecindario reducido (Candidate List Strategies)\n",
    "        #    Para reducir costo, sólo consideramos un subconjunto de vecinos.\n",
    "        vecinos = generar_vecindario(current)[:4]  # ejemplo: sólo primeros 4\n",
    "\n",
    "        # 3) Filtrado con Tabu y Criterio de Aspiración\n",
    "        candidatos = []\n",
    "        for v in vecinos:\n",
    "            is_tabu = v in tabu_list\n",
    "            score_v = evaluar_loss(v)  # pre-evaluamos breve para aspiración\n",
    "            # Aspiration: si un movimiento tabu mejora el best_score global, aceptamos\n",
    "            if not is_tabu or score_v < best_score:\n",
    "                candidatos.append((score_v, v, is_tabu))\n",
    "            # Actualizar LTM independientemente\n",
    "            lr = v['lr']\n",
    "            long_term['visits'][lr] = long_term['visits'].get(lr, 0) + 1\n",
    "            prev_best = long_term['best_scores'].get(lr, float('inf'))\n",
    "            long_term['best_scores'][lr] = min(prev_best, score_v)\n",
    "\n",
    "        # 4) Seleccionar mejor candidato\n",
    "        candidatos.sort(key=lambda x: x[0])\n",
    "        best_vec_score, best_vec, was_tabu = candidatos[0]\n",
    "\n",
    "        # 5) Actualizar estado\n",
    "        current = best_vec\n",
    "        # Si no se usó aspiración para este tabu, añadimos\n",
    "        if not was_tabu:\n",
    "            tabu_list.append(best_vec)\n",
    "            if len(tabu_list) > tabu_size:\n",
    "                tabu_list.pop(0)\n",
    "\n",
    "        # 6) Actualizar mejor global\n",
    "        if best_vec_score < best_score:\n",
    "            best_score = best_vec_score\n",
    "            best = best_vec.copy()\n",
    "\n",
    "        print(f\"Iter {i+1}/{iters} | LR={current['lr']:.6f} | Loss={best_vec_score:.4f} | MejGlobal={best_score:.4f}\")\n",
    "\n",
    "    # 7) Devolver mejor configuración, su score y memoria a largo plazo\n",
    "    return best, best_score, long_term\n",
    "\n",
    "# —————— Ejecución ——————\n",
    "if __name__ == \"__main__\":\n",
    "    mejor_conf, mejor_loss, memoria = tabu_search(\n",
    "        iters=15,\n",
    "        tabu_size=4,\n",
    "        initial_config={'lr': 0.001, 'num_epochs': 5}\n",
    "    )\n",
    "    print(\"\\n=== Mejor configuración encontrada ===\")\n",
    "    print(mejor_conf)\n",
    "    print(f\"Loss acumulado = {mejor_loss:.4f}\")\n",
    "    print(\"\\n=== Resumen de LONG-TERM MEMORY ===\")\n",
    "    print(memoria)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00b70b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Accuracy en test: 97.98%\n",
      "\n",
      "📋 Clasificación por clase:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       1.00      0.98      0.99        50\n",
      "           B       0.97      0.95      0.96        41\n",
      "           C       1.00      0.93      0.97        45\n",
      "           D       1.00      0.98      0.99        45\n",
      "           E       1.00      0.95      0.97        38\n",
      "           F       0.90      1.00      0.95        37\n",
      "           G       1.00      0.87      0.93        38\n",
      "           H       1.00      1.00      1.00        45\n",
      "           I       1.00      1.00      1.00        41\n",
      "           J       0.97      1.00      0.99        37\n",
      "           K       1.00      1.00      1.00        26\n",
      "           L       1.00      1.00      1.00        39\n",
      "           M       0.97      1.00      0.99        37\n",
      "           N       0.98      0.95      0.96        42\n",
      "           O       0.95      0.97      0.96        40\n",
      "           P       1.00      0.97      0.99        37\n",
      "           Q       0.96      1.00      0.98        43\n",
      "           R       0.98      1.00      0.99        40\n",
      "           S       0.79      1.00      0.88        26\n",
      "           T       1.00      1.00      1.00        48\n",
      "           U       1.00      1.00      1.00        34\n",
      "           V       0.98      0.98      0.98        49\n",
      "           W       1.00      1.00      1.00        45\n",
      "           X       1.00      1.00      1.00        36\n",
      "           Y       1.00      0.95      0.98        44\n",
      "           Z       1.00      1.00      1.00        37\n",
      "\n",
      "    accuracy                           0.98      1040\n",
      "   macro avg       0.98      0.98      0.98      1040\n",
      "weighted avg       0.98      0.98      0.98      1040\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "model.eval()  # Muy importante: desactiva dropout/batchnorm si los hubiera\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():  # No se calculan gradientes, más eficiente\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Métricas\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "print(f\"\\n✅ Accuracy en test: {acc * 100:.2f}%\")\n",
    "\n",
    "# Reporte más detallado\n",
    "print(\"\\n📋 Clasificación por clase:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=list(string.ascii_uppercase)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af4ccb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"letter_classifier.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1dc1592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "942043ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LetterClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=52):\n",
    "        super(LetterClassifier, self).__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),  # 1x128x128 → 32x128x128\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                             # 32x64x64\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), # 64x64x64\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                             # 64x32x32\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),# 128x32x32\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                             # 128x16x16\n",
    "        )\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(128 * 16 * 16, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc_layers(x)\n",
    "        return x  # logits (sin softmax, para usar con CrossEntropyLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c1b72d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iagoc\\AppData\\Local\\Temp\\ipykernel_5392\\1852476355.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  modelo.load_state_dict(torch.load('letter_classifier.pth', map_location='cpu'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LetterClassifier(\n",
       "  (conv_layers): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc_layers): Sequential(\n",
       "    (0): Linear(in_features=32768, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=52, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo = LetterClassifier()\n",
    "modelo.load_state_dict(torch.load('letter_classifier.pth', map_location='cpu'))\n",
    "modelo.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d670319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Letra predicha: A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iagoc\\AppData\\Local\\Temp\\ipykernel_5392\\2694318761.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  modelo.load_state_dict(torch.load(\"letter_classifier.pth\", map_location='cpu'))\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import string\n",
    "\n",
    "# Paso 1: Diccionario para convertir índices a letras\n",
    "idx_to_letter = list(string.ascii_uppercase + string.ascii_lowercase)\n",
    "\n",
    "# Paso 2: Cargar y transformar la imagen\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),                  # Asegura 1 canal\n",
    "    transforms.Resize((128, 128)),             # Tamaño esperado\n",
    "    transforms.ToTensor(),                   # Convertir a tensor [0,1]\n",
    "    transforms.Normalize((0.5,), (0.5,))     # Normalización opcional\n",
    "])\n",
    "\n",
    "# Paso 3: Cargar imagen\n",
    "img_path = \"arial_dataset/A_0.png\"\n",
    "imagen = Image.open(img_path)\n",
    "imagen = transform(imagen)\n",
    "imagen = imagen.unsqueeze(0)  # Añade dimensión de batch: [1, 1, 28, 28]\n",
    "\n",
    "# Paso 4: Cargar el modelo ya corregido\n",
    "modelo = LetterClassifier()\n",
    "modelo.load_state_dict(torch.load(\"letter_classifier.pth\", map_location='cpu'))\n",
    "modelo.eval()\n",
    "\n",
    "# Paso 5: Realizar predicción\n",
    "with torch.no_grad():\n",
    "    salida = modelo(imagen)\n",
    "    prediccion = torch.argmax(salida, dim=1).item()\n",
    "    letra = idx_to_letter[prediccion]\n",
    "\n",
    "print(f\"Letra predicha: {letra}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
